# This is the abkhazia configuration file. This file is automatically
# generated during installation. Change the values in here to overload
# the default configuration.

[abkhazia]
# The absolute path to the output data directory of abkhazia.
data-directory:

[kaldi]
# The absolute path to the kaldi distribution directory
# YOU MUST SPECIFY THIS PATH MANUALLY.
kaldi-directory:

# "queue.pl" uses qsub.  The options to it are
# options to qsub.  If you have GridEngine installed,
# change this to a queue you have access to.
# Otherwise, use "run.pl", which will run jobs locally
# (make sure your --num-jobs options are no more than
# the number of cpus on your machine.

# On Oberon use:
# train-cmd: queue.pl -q all.q@puck*.cm.cluster
# decode-cmd: queue.pl -q all.q@puck*.cm.cluster
# highmem-cmd: queue.pl -q all.q@puck*.cm.cluster

# On Eddie use:
# train-cmd: queue.pl -P inf_hcrc_cstr_general
# decode-cmd: queue.pl -P inf_hcrc_cstr_general
# highmem-cmd: queue.pl -P inf_hcrc_cstr_general -pe memory-2G 2

# To run locally use:
train-cmd: run.pl
decode-cmd: run.pl
highmem-cmd: run.pl

[corpus]
# In this section you can specify the default input directory where to
# read raw data for each supported corpus. By doing so, the
# <input-dir> argument of 'abkhazia prepare <corpus>' becomes optional
# for the corpus you have specified directories here.
aic-directory:
buckeye-directory:
childes-directory:
csj-directory:
globalphone-directory:
librispeech-directory:
wsj-directory:
xitsonga-directory:

[split]
# Defines the default proportion of data for the test set
default-test-proportion: 0.5

# Could be useful when using a lexicon that is tailored to the corpus
# to the point of overfitting (i.e. only words occuring in the corpus
# were included and many other common words weren't), which could lead
# to overestimated performance on words from the lexicon appearing in
# the test only. Removes from the lexicon all words that are not
# present at least once in the training set.
prune-lexicon: false

[features]
# Compute pitch features along with MFCCs
use-pitch: true

[language]
# Should be set to true or false depending on whether the language
# model produced is destined to be used with an acoustic model trained
# with or without word position dependent variants of the phones
word-position-dependent: true

# Do all computations if true, else focus on the main ones
optional-silence: true

# n in n-gram
model-order: 3

# Can be either 'phone' or 'word' and compute either a phone-level or
# a word-level language model
model-level: word

[acoustic]
# Speaker-independent triphone models parameters
num-states-si: 2500
num-gauss-si: 15000

# Speaker-adaptive triphone models parameters
num-states-sa: 2500
num-gauss-sa: 15000

[decode]
# acoustic scale for extracting posterior from the final lattice
acoustic-scale: 0.1
