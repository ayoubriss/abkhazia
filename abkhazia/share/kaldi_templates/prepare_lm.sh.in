#!/bin/bash -u
# Copyright 2016 Thomas Schatz, Xuan Nga Cao, Mathieu Bernard
#
# This file is part of abkhazia: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Abkhazia is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with abkahzia. If not, see <http://www.gnu.org/licenses/>.

# TODO explain here what the recipe does

###### Parameters ######

# Name of the LM, must correspond to an existing folder in data/
# (created by the python script generating the recipe)
name=$1

# n in n-gram, only used if a LM is to be estimated from some text
# (see below). Only tested with n=2
model_order=@@@@

# Should be set to true or false depending on whether the language
# model produced is destined to be used with an acoustic model trained
# with or without word position dependent variants of the phones.
word_position_dependent=@@@@

# do all computation or focus on main ones
optional_silence=@@@@


###### Recipe ######

# directory containing all the info about the desired lm
in_dir=data/local/$name

# output directory
out_dir=data/$name

# log file
log=data/prepare_"$name".log


[ -f cmd.sh ] && source ./cmd.sh \
  || echo "cmd.sh not found. Jobs may not execute properly."

. path.sh || { echo "Cannot source path.sh"; exit 1; }

# First need to do a prepare_lang in the desired folder to get to use
# the right "phone" or "word" lexicon irrespective of what was used as
# a lexicon in training. If word_position_dependent is true and the
# lm is at the phone level, use prepare_lang_wpdpl.sh in the local
# folder, otherwise we fall back to the original utils/prepare_lang.sh
# (some slight customizations of the script are necessary to decode
# with a phone loop language model when word position dependent phone
# variants have been trained).
if [ "$word_position_dependent" = true -a  -f "$in_dir"/phone ]
then
    # empty phone file is used to signal if the LM is not at the word
    # but at the phone level
    prepare_lang_exe=local/prepare_lang_wpdpl.sh
else
    prepare_lang_exe=utils/prepare_lang.sh
fi

if [ "$optional_silence" = true ] ; then
    sil_prob=0.5
else
    sil_prob=0.0
fi

echo "running $prepare_lang_exe..."

# temp directory for language preparation
tmp_dir=$(mktemp -d /tmp/kaldi.XXXX);
trap 'rm -rf "$tmp_dir"' EXIT

$prepare_lang_exe \
    --position-dependent-phones $word_position_dependent \
    --sil_prob $sil_prob \
    $in_dir "<unk>" $tmp_dir $out_dir \
    >& $log || exit 1


# here we could use a different silence_prob. I think however that
# --position-dependent-phones has to be the same as what was used for
# training (as well as the phones.txt, extra_questions.txt and
# nonsilence_phones.txt), otherwise the mapping between phones and
# acoustic state in the trained model will be lost

# Next three possibilities:
#  1 - A G.txt file is already provided in in_dir (FST grammar in text format)
#  2 - A G.arpa.gz MIT/ARPA formatted n-gram is already provided in in_dir
#  3 - A text.txt file from which to estimate a n-gram is provided in in_dir

if [ -f "$in_dir"/G.txt ]
then
    echo "compiling $in_dir/G.txt to $out_dir/G.fst"
    # 1 - compile the text format FST to binary format used by kaldi
    # in utils/mkgraph.sh
    fstcompile --isymbols=$out_dir/words.txt \
               --osymbols=$out_dir/words.txt \
               --keep_isymbols=false \
	       --keep_osymbols=false \
               $in_dir/G.txt > $out_dir/G.fst

    # sort G.fst for computational efficiency (I think)
    fstarcsort --sort_type=ilabel \
               $out_dir/G.fst > $out_dir/G2.fst

    mv $out_dir/G2.fst $out_dir/G.fst
else
    if [ ! -f "$in_dir"/G.arpa.gz ]
    then
        echo "creating $in_dir/G.arpa.gz"

        # generate ARPA/MIT n-gram with IRSTLM, then as in 2. train
        # (use IRSTLM) need to remove utt-id on first column of text file
        set -eu  # stop on error
        cut -d' ' -f2- < "$in_dir"/lm_text.txt > "$in_dir"/text_ready.txt
        add-start-end.sh < "$in_dir"/text_ready.txt > "$in_dir"/text_se.txt

        # k option is number of split, useful for huge text files
        # build-lm.sh in kaldi/tools/irstlm/bin
        build-lm.sh -i "$in_dir"/text_se.txt -n $model_order \
                    -o "$in_dir"/text.ilm.gz -k 1 -s kneser-ney >> $log

        compile-lm "$in_dir"/text.ilm.gz --text=yes /dev/stdout 2>> $log | \
            gzip -c > "$in_dir"/G.arpa.gz

        # clean intermediate files
        rm -f "$in_dir"/text_ready.txt
        rm -f "$in_dir"/text_se.txt
        rm -f "$in_dir"/text.ilm.gz
    fi

    echo "creating language model in $out_dir/G.fst"

    # 2 - generate FST (use SRILM)
    # includes adapting the vocabulary to lexicon in out_dir
    # srilm_opts: do not use -tolower by default, since we do not
    # make assumption that lexicon has no meaningful
    # lowercase/uppercase distinctions (and it can be in unicode,
    # in which case I have no idea what lowercasing would produce)

    # format_lm_sri.sh copies stuff so we need to instantiate
    # another folder and then clean up (or we could do a custom
    # format_lm_sri.sh with $1 and $4 == $1 and no cp)
    tmpdir=$(mktemp -d /tmp/kaldi.XXXX);
    trap 'rm -rf "$tmpdir"' EXIT

    utils/format_lm_sri.sh --srilm_opts "-subset -prune-lowprobs -unk" \
			   $out_dir "$in_dir"/G.arpa.gz \
			   "in_dir"/lexicon.txt $tmpdir &>> $log
    rm -Rf $out_dir  # erases the previous content that is redundant anyway
    mv $tmpdir $out_dir
fi
